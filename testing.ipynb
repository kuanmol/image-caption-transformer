{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-06T06:51:36.403450Z",
     "start_time": "2025-08-06T06:51:28.552704Z"
    }
   },
   "source": [
    "import torch\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def print_gpu_info():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_id = torch.cuda.current_device()\n",
    "        print(f\"\\n[INFO] Using GPU: {torch.cuda.get_device_name(gpu_id)}\")\n",
    "\n",
    "        allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 2)  # in MB\n",
    "        reserved = torch.cuda.memory_reserved(gpu_id) / (1024 ** 2)    # in MB\n",
    "        total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / (1024 ** 2)  # in MB\n",
    "\n",
    "        print(f\"[MEMORY] Allocated: {allocated:.2f} MB\")\n",
    "        print(f\"[MEMORY] Reserved : {reserved:.2f} MB\")\n",
    "        print(f\"[MEMORY] Total    : {total_mem:.2f} MB\")\n",
    "\n",
    "        # Run nvidia-smi for detailed live stats (if available)\n",
    "        print(\"\\n[GPU STATUS - nvidia-smi]\")\n",
    "        try:\n",
    "            subprocess.run([\"nvidia-smi\"], check=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not run nvidia-smi: {e}\")\n",
    "    else:\n",
    "        print(\"[INFO] CUDA not available.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print_gpu_info()\n",
    "\n",
    "    # Dummy PyTorch usage to simulate GPU load\n",
    "    a = torch.rand((10000, 10000), device='cuda')\n",
    "    b = torch.matmul(a, a)\n",
    "    print_gpu_info()\n",
    "\n",
    "    # Optional: Wait and monitor changes\n",
    "    time.sleep(5)\n",
    "    print_gpu_info()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "[MEMORY] Allocated: 0.00 MB\n",
      "[MEMORY] Reserved : 0.00 MB\n",
      "[MEMORY] Total    : 6143.50 MB\n",
      "\n",
      "[GPU STATUS - nvidia-smi]\n",
      "\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "[MEMORY] Allocated: 772.12 MB\n",
      "[MEMORY] Reserved : 784.00 MB\n",
      "[MEMORY] Total    : 6143.50 MB\n",
      "\n",
      "[GPU STATUS - nvidia-smi]\n",
      "\n",
      "[INFO] Using GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "[MEMORY] Allocated: 772.12 MB\n",
      "[MEMORY] Reserved : 784.00 MB\n",
      "[MEMORY] Total    : 6143.50 MB\n",
      "\n",
      "[GPU STATUS - nvidia-smi]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HF_TOKEN\")"
   ],
   "id": "eda8acd776f0c9ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import os\n",
    "\n",
    "# Set environment variable to avoid potential conflicts\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# Load a smaller dataset for summarization\n",
    "dataset = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# results = metric.compute(predictions=predictions, references=references)\n",
    "# print(results)\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ],
   "id": "174b7409c5cdf12a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5745aa0a4efd9623"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "521c03c7aa4bc018"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16bdb8d453e19ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2234e230c3d260c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "23b585f191c5bf37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "957dce16ac221df9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fc9871be5bccfd35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dbce0a5b23a1c491"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e394d6d3670afad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T04:00:47.880277500Z",
     "start_time": "2025-07-07T03:56:52.731160Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c2e33a8cb2ca5cec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU, Memory: 6.44 GB\n",
      "Initial GPU Memory Allocated: 0.00 MB\n",
      "CPU Usage: 20.7%, RAM Free: 4.59 GB\n",
      "File exists: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features\\train_vit_features.pt, Size: 412.90 MB\n",
      "File exists: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features\\val_vit_features.pt, Size: 51.78 MB\n",
      "File exists: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions.pkl, Size: 14.01 MB\n",
      "Initializing datasets...\n",
      "Loading feature file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features\\train_vit_features.pt\n",
      "Features shape: torch.Size([127132, 768]), Image names: 127132, Loaded in 0.60 seconds\n",
      "Loading caption file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions.pkl\n",
      "Captions loaded: 158914 captions in 0.37 seconds\n",
      "Building vocabulary...\n",
      "Vocabulary built in 0.19 seconds, size: 12097\n",
      "Loading feature file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features\\val_vit_features.pt\n",
      "Features shape: torch.Size([15891, 768]), Image names: 15891, Loaded in 0.05 seconds\n",
      "Loading caption file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions.pkl\n",
      "Captions loaded: 158914 captions in 0.62 seconds\n",
      "Building vocabulary...\n",
      "Vocabulary built in 0.16 seconds, size: 12097\n",
      "Loading feature file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features\\test_vit_features.pt\n",
      "Features shape: torch.Size([15892, 768]), Image names: 15892, Loaded in 0.05 seconds\n",
      "Loading caption file: D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions.pkl\n",
      "Captions loaded: 158914 captions in 0.49 seconds\n",
      "Building vocabulary...\n",
      "Vocabulary built in 0.17 seconds, size: 12097\n",
      "Datasets initialized in 2.78 seconds\n",
      "DataLoader initialized with num_workers=2\n",
      "Saved vocab.pkl with 12097 tokens, Size: 0.25 MB\n",
      "Model initialized, GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1e6:.2f} MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/3973 [00:00<?, ?it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:46:38.592205Z",
     "start_time": "2025-07-07T13:46:32.958772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption in captions for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "# Dataset\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.captions = pickle.load(open(caption_file, 'rb'))\n",
    "        self.image_names = data['image_names']\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        caption = self.captions[idx]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), self.image_names[idx]\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names"
   ],
   "id": "1519dbdfbe414fdc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:46:48.342101Z",
     "start_time": "2025-07-07T13:46:42.147782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Bi-GRU Model\n",
    "class BiGRUModel(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2, dropout=0.5):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, hidden_dim)  # Maps 768 -> 512\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)  # Output from bidirectional GRU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def get_initial_hidden(self, features):\n",
    "        \"\"\"Initialize hidden state using transformed image features.\"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        # features is already [batch_size, hidden_dim] after feature_fc\n",
    "        h_first_layer = features.unsqueeze(0).repeat(2, 1, 1)  # [2, batch_size, hidden_dim] for first layer (fwd + bwd)\n",
    "        h_second_layer = torch.zeros(2, batch_size, self.hidden_dim).to(features.device)  # [2, batch_size, hidden_dim] for second layer\n",
    "        hidden = torch.cat([h_first_layer, h_second_layer], dim=0)  # [4, batch_size, hidden_dim]\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"Forward pass for training with teacher forcing.\"\"\"\n",
    "        # Transform features once\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))  # [batch_size, 512]\n",
    "        batch_size = features.size(0)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "\n",
    "        # Initialize hidden state\n",
    "        hidden = self.get_initial_hidden(features)  # [4, batch_size, 512]\n",
    "        embed = self.embedding(captions[:, :-1])  # Exclude <EOS> for input\n",
    "        outputs = []\n",
    "        input_token = captions[:, 0].unsqueeze(1)  # Start with <SOS>\n",
    "\n",
    "        for t in range(embed.size(1)):\n",
    "            embed_input = self.embedding(input_token).squeeze(1)  # [batch_size, embed_dim]\n",
    "            _, hidden = self.gru(embed_input.unsqueeze(1), hidden)  # Update hidden state\n",
    "            output = self.fc(hidden[-2:].transpose(0, 1).contiguous().view(batch_size, -1))  # [batch_size, vocab_size]\n",
    "            outputs.append(output)\n",
    "            # Teacher forcing\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                input_token = captions[:, t + 1].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = output.topk(1)\n",
    "                input_token = topi.detach()\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, features, max_len=20):\n",
    "        \"\"\"Inference mode for generating captions.\"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))  # [batch_size, 512]\n",
    "        hidden = self.get_initial_hidden(features)  # [4, batch_size, 512]\n",
    "        input_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=features.device)  # <SOS>\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            embed = self.embedding(input_token).squeeze(1)  # [batch_size, embed_dim]\n",
    "            _, hidden = self.gru(embed.unsqueeze(1), hidden)  # Update hidden state\n",
    "            output = self.fc(hidden[-2:].transpose(0, 1).contiguous().view(batch_size, -1))  # [batch_size, vocab_size]\n",
    "            _, next_token = output.max(1)  # Greedy decoding\n",
    "            outputs.append(next_token)\n",
    "            input_token = next_token.unsqueeze(1)\n",
    "            if (next_token == 2).all():  # Stop at <EOS>\n",
    "                break\n",
    "\n",
    "        return torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'train_vit_features.pt'), caption_file)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Save vocab for evaluation\n",
    "with open(os.path.join(feature_dir, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_dataset.vocab, f)\n",
    "print(f\"Saved vocab.pkl with {len(train_dataset.vocab.itos)} tokens\")\n",
    "\n",
    "# Model\n",
    "model = BiGRUModel(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(train_dataset.vocab.itos),\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").cuda()\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, min_lr=1e-6, verbose=True)\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, captions, _ in tqdm(loader, desc=\"Training\"):\n",
    "        if features is None:\n",
    "            continue\n",
    "        features, captions = features.cuda(), captions.cuda()\n",
    "        outputs = model(features, captions, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, captions, _ in tqdm(loader, desc=\"Validating\"):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model(features, captions, teacher_forcing_ratio=1.0)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_count = 0\n",
    "min_delta = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {(time.time() - start_time) / 60:.2f} min, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), os.path.join(feature_dir, 'best_bigru_model1.pt'))\n",
    "        print(f\"Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f\"No improvement in Val Loss, count: {no_improve_count}/{patience}\")\n",
    "    scheduler.step(val_loss)  # Update learning rate based on val_loss\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n"
   ],
   "id": "d300475705132f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab.pkl with 18 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1987 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "124693",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 148\u001B[39m\n\u001B[32m    146\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[32m    147\u001B[39m     start_time = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m148\u001B[39m     train_loss = \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mteacher_forcing_ratio\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    149\u001B[39m     val_loss = validate_epoch(model, val_loader, criterion)\n\u001B[32m    150\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Val Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(time.time()\u001B[38;5;250m \u001B[39m-\u001B[38;5;250m \u001B[39mstart_time)\u001B[38;5;250m \u001B[39m/\u001B[38;5;250m \u001B[39m\u001B[32m60\u001B[39m\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m min, LR: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moptimizer.param_groups[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mlr\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.6f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 114\u001B[39m, in \u001B[36mtrain_epoch\u001B[39m\u001B[34m(model, loader, criterion, optimizer, teacher_forcing_ratio)\u001B[39m\n\u001B[32m    112\u001B[39m model.train()\n\u001B[32m    113\u001B[39m total_loss = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mTraining\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mcontinue\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    698\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    699\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    700\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m701\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    702\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    703\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    704\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    705\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    706\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    707\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    755\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    756\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m757\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    758\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    759\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Projects\\AIniverse\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 48\u001B[39m, in \u001B[36mFlickr30kCaptionDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[32m     47\u001B[39m     feature = \u001B[38;5;28mself\u001B[39m.features[idx]\n\u001B[32m---> \u001B[39m\u001B[32m48\u001B[39m     caption = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcaptions\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     49\u001B[39m     numerical_caption = [\u001B[38;5;28mself\u001B[39m.vocab.stoi[\u001B[33m'\u001B[39m\u001B[33m<SOS>\u001B[39m\u001B[33m'\u001B[39m]] + \u001B[38;5;28mself\u001B[39m.vocab.numericalize(caption) + [\u001B[38;5;28mself\u001B[39m.vocab.stoi[\u001B[33m'\u001B[39m\u001B[33m<EOS>\u001B[39m\u001B[33m'\u001B[39m]]\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m feature, torch.tensor(numerical_caption, dtype=torch.long), \u001B[38;5;28mself\u001B[39m.image_names[idx]\n",
      "\u001B[31mKeyError\u001B[39m: 124693"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:26:18.565908Z",
     "start_time": "2025-07-07T13:25:15.332107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "\n",
    "# Vocabulary class (unchanged)\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption in captions for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "# Dataset (unchanged)\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, vocab=None):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.captions = pickle.load(open(caption_file, 'rb'))\n",
    "        self.image_names = data['image_names']\n",
    "\n",
    "        # Use external vocab if provided\n",
    "        if vocab is not None:\n",
    "            self.vocab = vocab\n",
    "        else:\n",
    "            self.vocab = Vocabulary(min_freq=2)\n",
    "            self.vocab.build_vocabulary(self.captions)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        caption = self.captions[image_name]\n",
    "\n",
    "    # Fix for multiple captions per image\n",
    "        if isinstance(caption[0], list):\n",
    "            caption = caption[0]  # Pick the first caption\n",
    "\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "\n",
    "\n",
    "# Custom collate function (unchanged)\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)  # [batch_size, 768]\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names\n",
    "\n",
    "# Bi-GRU Model (unchanged, for loading)\n",
    "class BiGRUModel(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2, dropout=0.5):\n",
    "        super(BiGRUModel, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, hidden_dim)  # Maps 768 -> 512\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, vocab_size)  # Output from bidirectional GRU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def get_initial_hidden(self, features):\n",
    "        \"\"\"Initialize hidden state using transformed image features.\"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        h_first_layer = features.unsqueeze(0).repeat(2, 1, 1)  # [2, batch_size, hidden_dim] for first layer (fwd + bwd)\n",
    "        h_second_layer = torch.zeros(2, batch_size, self.hidden_dim).to(features.device)  # [2, batch_size, hidden_dim] for second layer\n",
    "        hidden = torch.cat([h_first_layer, h_second_layer], dim=0)  # [4, batch_size, hidden_dim]\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"Forward pass for training with teacher forcing.\"\"\"\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))  # [batch_size, 512]\n",
    "        batch_size = features.size(0)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        hidden = self.get_initial_hidden(features)  # [4, batch_size, 512]\n",
    "        embed = self.embedding(captions[:, :-1])  # Exclude <EOS> for input\n",
    "        outputs = []\n",
    "        input_token = captions[:, 0].unsqueeze(1)  # Start with <SOS>\n",
    "        for t in range(embed.size(1)):\n",
    "            embed_input = self.embedding(input_token).squeeze(1)  # [batch_size, embed_dim]\n",
    "            _, hidden = self.gru(embed_input.unsqueeze(1), hidden)  # Update hidden state\n",
    "            output = self.fc(hidden[-2:].transpose(0, 1).contiguous().view(batch_size, -1))  # [batch_size, vocab_size]\n",
    "            outputs.append(output)\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                input_token = captions[:, t + 1].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = output.topk(1)\n",
    "                input_token = topi.detach()\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch_size, seq_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def inference(self, features, max_len=20):\n",
    "        \"\"\"Inference mode for generating captions.\"\"\"\n",
    "        batch_size = features.size(0)\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))  # [batch_size, 512]\n",
    "        hidden = self.get_initial_hidden(features)  # [4, batch_size, 512]\n",
    "        input_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=features.device)  # <SOS>\n",
    "        outputs = []\n",
    "        for _ in range(max_len):\n",
    "            embed = self.embedding(input_token).squeeze(1)  # [batch_size, embed_dim]\n",
    "            _, hidden = self.gru(embed.unsqueeze(1), hidden)  # Update hidden state\n",
    "            output = self.fc(hidden[-2:].transpose(0, 1).contiguous().view(batch_size, -1))  # [batch_size, vocab_size]\n",
    "            _, next_token = output.max(1)  # Greedy decoding\n",
    "            outputs.append(next_token)\n",
    "            input_token = next_token.unsqueeze(1)\n",
    "            if (next_token == 2).all():  # Stop at <EOS>\n",
    "                break\n",
    "        return torch.stack(outputs, dim=1)  # [batch_size, seq_len]\n",
    "\n",
    "# Paths (updated for Colab, assuming files are in Google Drive)\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load vocabulary\n",
    "\n",
    "# Load the correct vocab used during training\n",
    "# Load training vocab\n",
    "vocab_path = os.path.join(feature_dir, 'vocab.pkl')\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Pass same vocab to both datasets\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, vocab)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, vocab)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = BiGRUModel(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(vocab.itos),\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").cuda()\n",
    "\n",
    "# Load trained model weights\n",
    "model_path = os.path.join(feature_dir, 'best_bigru_model1.pt')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, vocab, max_len=20, num_samples=5):\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len)  # [batch_size, seq_len]\n",
    "\n",
    "            for i in range(captions.size(0)):\n",
    "                # Ground truth caption (remove <SOS>, <EOS>, <PAD>)\n",
    "                ref = [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]]\n",
    "                # Generated caption (remove <SOS>, <EOS>, <PAD>)\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])  # BLEU expects list of reference lists\n",
    "                hypotheses.append(hyp)\n",
    "\n",
    "                # Store samples for qualitative inspection\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_names[i],\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "\n",
    "    # Compute BLEU-4 with smoothing\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "\n",
    "    return bleu_score, sample_outputs\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_bleu, val_samples = evaluate_model(model, val_loader, vocab, max_len=20, num_samples=5)\n",
    "test_bleu, test_samples = evaluate_model(model, test_loader, vocab, max_len=20, num_samples=5)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nValidation BLEU-4 Score: {val_bleu:.4f}\")\n",
    "print(\"\\nValidation Sample Outputs:\")\n",
    "for sample in val_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")\n",
    "\n",
    "print(f\"\\nTest BLEU-4 Score: {test_bleu:.4f}\")\n",
    "print(\"\\nTest Sample Outputs:\")\n",
    "for sample in test_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")"
   ],
   "id": "4792f89298b1f09",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 249/249 [00:25<00:00,  9.79it/s]\n",
      "Evaluating: 100%|██████████| 249/249 [00:33<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation BLEU-4 Score: 0.0000\n",
      "\n",
      "Validation Sample Outputs:\n",
      "Image: 4420802292.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt and a a a a a\n",
      "\n",
      "Image: 1579291454.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt a a a\n",
      "\n",
      "Image: 3720366614.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt is a a a\n",
      "\n",
      "Image: 2315113960.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt and a a a a\n",
      "\n",
      "Image: 2785408815.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt is a a a a\n",
      "\n",
      "\n",
      "Test BLEU-4 Score: 0.0000\n",
      "\n",
      "Test Sample Outputs:\n",
      "Image: 3692746368.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt a a a\n",
      "\n",
      "Image: 157139628.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt is a a a a\n",
      "\n",
      "Image: 3205008852.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt a a a\n",
      "\n",
      "Image: 2589156742.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt and a a a a\n",
      "\n",
      "Image: 2335129954.jpg\n",
      "Ground Truth: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Generated: a man in a a shirt is a a a a\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:14:43.819497Z",
     "start_time": "2025-07-07T13:11:32.431240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocess captions\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    captions = [str(c).strip() for c in captions if not pd.isna(c)]\n",
    "    results = []\n",
    "    for i in range(0, len(captions), batch_size):\n",
    "        batch = captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([caption.lower() for caption in batch], disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "    return results\n",
    "\n",
    "# Paths\n",
    "caption_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "output_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "\n",
    "# Read captions with UTF-8 and error handling, skip header\n",
    "try:\n",
    "    df = pd.read_csv(caption_file, sep=',', names=['image', 'caption'], skiprows=1, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(caption_file, sep=',', names=['image', 'caption'], skiprows=1, encoding='latin1')\n",
    "\n",
    "# Preprocess captions and create image-caption dictionary\n",
    "image_captions = {}\n",
    "captions = df['caption'].tolist()\n",
    "image_names = df['image'].tolist()\n",
    "preprocessed_captions = preprocess_caption(captions)\n",
    "\n",
    "for img, cap in zip(image_names, preprocessed_captions):\n",
    "    if img not in image_captions:\n",
    "        image_captions[img] = []\n",
    "    image_captions[img].append(cap)\n",
    "\n",
    "# Save preprocessed captions\n",
    "pickle.dump(image_captions, open(output_file, 'wb'))\n",
    "print(f\"Saved preprocessed_captions.pkl with {len(image_captions)} images\")\n",
    "\n",
    "# Debug: Verify alignment\n",
    "print(\"\\nSample Preprocessed Captions:\")\n",
    "for i, (img, caps) in enumerate(list(image_captions.items())[:5]):\n",
    "    print(f\"Image {i+1}: {img}, Captions: {caps}\")"
   ],
   "id": "53944b6c7f8e5c20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed_captions.pkl with 31783 images\n",
      "\n",
      "Sample Preprocessed Captions:\n",
      "Image 1: 1000092795.jpg, Captions: [['two', 'young', 'guys', 'with', 'shaggy', 'hair', 'look', 'at', 'their', 'hands', 'while', 'hanging', 'out', 'in', 'the', 'yard'], ['two', 'young', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes'], ['two', 'men', 'in', 'green', 'shirts', 'are', 'standing', 'in', 'a', 'yard'], ['a', 'man', 'in', 'a', 'blue', 'shirt', 'standing', 'in', 'a', 'garden'], ['two', 'friends', 'enjoy', 'time', 'spent', 'together']]\n",
      "Image 2: 10002456.jpg, Captions: [['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system'], ['workers', 'look', 'down', 'from', 'up', 'above', 'on', 'a', 'piece', 'of', 'equipment'], ['two', 'men', 'working', 'on', 'a', 'machine', 'wearing', 'hard', 'hats'], ['four', 'men', 'on', 'top', 'of', 'a', 'tall', 'structure'], ['three', 'men', 'on', 'a', 'large', 'rig']]\n",
      "Image 3: 1000268201.jpg, Captions: [['a', 'child', 'in', 'a', 'pink', 'dress', 'is', 'climbing', 'up', 'a', 'set', 'of', 'stairs', 'in', 'an', 'entry', 'way'], ['a', 'little', 'girl', 'in', 'a', 'pink', 'dress', 'going', 'into', 'a', 'wooden', 'cabin'], ['a', 'little', 'girl', 'climbing', 'the', 'stairs', 'to', 'her', 'playhouse'], ['a', 'little', 'girl', 'climbing', 'into', 'a', 'wooden', 'playhouse'], ['a', 'girl', 'going', 'into', 'a', 'wooden', 'building']]\n",
      "Image 4: 1000344755.jpg, Captions: [['someone', 'in', 'a', 'blue', 'shirt', 'and', 'hat', 'is', 'standing', 'on', 'stair', 'and', 'leaning', 'against', 'a', 'window'], ['a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window'], ['a', 'man', 'on', 'a', 'ladder', 'cleans', 'the', 'window', 'of', 'a', 'tall', 'building'], ['man', 'in', 'blue', 'shirt', 'and', 'jeans', 'on', 'ladder', 'cleaning', 'windows'], ['a', 'man', 'on', 'a', 'ladder', 'cleans', 'a', 'window']]\n",
      "Image 5: 1000366164.jpg, Captions: [['two', 'men', 'one', 'in', 'a', 'gray', 'shirt', 'one', 'in', 'a', 'black', 'shirt', 'standing', 'near', 'a', 'stove'], ['two', 'guy', 'cooking', 'and', 'joking', 'around', 'with', 'the', 'camera'], ['two', 'men', 'in', 'a', 'kitchen', 'cooking', 'food', 'on', 'a', 'stove'], ['two', 'men', 'are', 'at', 'the', 'stove', 'preparing', 'food'], ['two', 'men', 'are', 'cooking', 'a', 'meal']]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2cc1da580dab4dcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T07:49:50.210519Z",
     "start_time": "2025-07-08T03:37:24.658209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption_list in captions.values() for caption in caption_list for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "# Dataset\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, augment=False):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.image_names = data['image_names']\n",
    "        self.captions_dict = pickle.load(open(caption_file, 'rb'))\n",
    "        self.augment = augment\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        captions = self.captions_dict.get(image_name, [[]])\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()] if self.augment and len(captions) > 1 else captions[0]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names\n",
    "\n",
    "# Transformer Model with Beam Search\n",
    "class ImageCaptionTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, hidden_dim=2048, dropout=0.05, max_len=100):\n",
    "        super(ImageCaptionTransformer, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding(captions) * (self.embed_dim ** 0.5)\n",
    "        positions = torch.arange(0, captions.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embed = self.pos_encoder[:, :captions.size(1), :]\n",
    "        embed = embed + pos_embed\n",
    "        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
    "        output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def inference(self, features, max_len=20, beam_size=3):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        sequences = [[torch.full((1, 1), 1, dtype=torch.long, device=device), 0.0]] * batch_size\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for i in range(batch_size):\n",
    "                candidates = []\n",
    "                for seq, score in sequences[i]:\n",
    "                    if seq[0, -1].item() == 2:\n",
    "                        candidates.append([seq, score])\n",
    "                        continue\n",
    "                    embed = self.embedding(seq) * (self.embed_dim ** 0.5)\n",
    "                    positions = torch.arange(0, seq.size(1), device=device).unsqueeze(0)\n",
    "                    pos_embed = self.pos_encoder[:, :seq.size(1), :]\n",
    "                    embed = embed + pos_embed\n",
    "                    tgt_mask = self.generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "                    output = self.transformer_decoder(embed, memory[i:i+1], tgt_mask=tgt_mask)\n",
    "                    output = self.fc(output[:, -1, :])\n",
    "                    probs = torch.softmax(output, dim=-1)\n",
    "                    top_probs, top_idx = probs.topk(beam_size, dim=-1)\n",
    "                    for k in range(beam_size):\n",
    "                        next_token = top_idx[0, k].unsqueeze(0).unsqueeze(0)\n",
    "                        next_score = score - torch.log(top_probs[0, k]).item()\n",
    "                        new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                        candidates.append([new_seq, next_score])\n",
    "                candidates = sorted(candidates, key=lambda x: x[1])[:beam_size]\n",
    "                all_candidates.append(candidates)\n",
    "            sequences = all_candidates\n",
    "        outputs = [sequences[i][0][0] for i in range(batch_size)]\n",
    "        return torch.cat(outputs, dim=0)\n",
    "\n",
    "# Load original captions\n",
    "def load_original_captions(caption_txt_file):\n",
    "    caption_dict = {}\n",
    "    try:\n",
    "        with open(caption_txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(caption_txt_file, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    return caption_dict\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, vocab, caption_dict, max_len=20, num_samples=5, beam_size=3):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len, beam_size=beam_size)\n",
    "            for i in range(captions.size(0)):\n",
    "                image_name = image_names[i]\n",
    "                ref = caption_dict.get(image_name, [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]])\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_name,\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    return bleu_score, sample_outputs\n",
    "\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "caption_txt_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'train_vit_features.pt'), caption_file, augment=True)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, augment=False)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Save vocab\n",
    "with open(os.path.join(feature_dir, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_dataset.vocab, f)\n",
    "print(f\"Saved vocab.pkl with {len(train_dataset.vocab.itos)} tokens\")\n",
    "\n",
    "# Model\n",
    "model = ImageCaptionTransformer(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(train_dataset.vocab.itos),\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    hidden_dim=2048,\n",
    "    dropout=0.05,\n",
    "    max_len=100\n",
    ").cuda()\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, captions, _ in tqdm(loader, desc=\"Training\"):\n",
    "        if features is None:\n",
    "            continue\n",
    "        features, captions = features.cuda(), captions.cuda()\n",
    "        outputs = model(features, captions, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, captions, _ in tqdm(loader, desc=\"Validating\"):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model(features, captions, teacher_forcing_ratio=1.0)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_count = 0\n",
    "min_delta = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {(time.time() - start_time) / 60:.2f} min, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), os.path.join(feature_dir, 'best_transformer_model.pt'))\n",
    "        print(f\"Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f\"No improvement in Val Loss, count: {no_improve_count}/{patience}\")\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break"
   ],
   "id": "f4a32b6380785877",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved vocab.pkl with 12097 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:10<00:00,  9.23it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 4.6521, Val Loss: 4.2158, Time: 7.39 min, LR: 0.000100\n",
      "Saved best model with Val Loss: 4.2158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:31<00:00,  8.81it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 4.1017, Val Loss: 3.9552, Time: 7.74 min, LR: 0.000099\n",
      "Saved best model with Val Loss: 3.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:16<00:00,  9.11it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Train Loss: 3.9127, Val Loss: 3.8118, Time: 7.47 min, LR: 0.000098\n",
      "Saved best model with Val Loss: 3.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:11<00:00,  9.21it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Train Loss: 3.7773, Val Loss: 3.6878, Time: 7.40 min, LR: 0.000096\n",
      "Saved best model with Val Loss: 3.6878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:13<00:00,  9.16it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Train Loss: 3.6656, Val Loss: 3.5878, Time: 7.43 min, LR: 0.000093\n",
      "Saved best model with Val Loss: 3.5878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:16<00:00,  9.11it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Train Loss: 3.5630, Val Loss: 3.5023, Time: 7.47 min, LR: 0.000091\n",
      "Saved best model with Val Loss: 3.5023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:15<00:00,  9.12it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Train Loss: 3.4722, Val Loss: 3.4153, Time: 7.47 min, LR: 0.000087\n",
      "Saved best model with Val Loss: 3.4153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:14<00:00,  9.14it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Train Loss: 3.3891, Val Loss: 3.3371, Time: 7.44 min, LR: 0.000084\n",
      "Saved best model with Val Loss: 3.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:24<00:00,  8.94it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Train Loss: 3.3070, Val Loss: 3.2566, Time: 7.61 min, LR: 0.000080\n",
      "Saved best model with Val Loss: 3.2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:38<00:00,  8.66it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Train Loss: 3.2314, Val Loss: 3.1775, Time: 7.87 min, LR: 0.000075\n",
      "Saved best model with Val Loss: 3.1775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:29<00:00,  8.84it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 36.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Train Loss: 3.1508, Val Loss: 3.1093, Time: 7.72 min, LR: 0.000071\n",
      "Saved best model with Val Loss: 3.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:29<00:00,  8.84it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Train Loss: 3.0766, Val Loss: 3.0308, Time: 7.71 min, LR: 0.000066\n",
      "Saved best model with Val Loss: 3.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:35<00:00,  8.73it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 36.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Train Loss: 3.0090, Val Loss: 2.9622, Time: 7.81 min, LR: 0.000061\n",
      "Saved best model with Val Loss: 2.9622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:26<00:00,  8.91it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 38.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Train Loss: 2.9370, Val Loss: 2.8978, Time: 7.65 min, LR: 0.000056\n",
      "Saved best model with Val Loss: 2.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:29<00:00,  8.84it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Train Loss: 2.8738, Val Loss: 2.8329, Time: 7.71 min, LR: 0.000051\n",
      "Saved best model with Val Loss: 2.8329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:30<00:00,  8.82it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Train Loss: 2.8078, Val Loss: 2.7725, Time: 7.73 min, LR: 0.000045\n",
      "Saved best model with Val Loss: 2.7725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:25<00:00,  8.92it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Train Loss: 2.7514, Val Loss: 2.7124, Time: 7.63 min, LR: 0.000040\n",
      "Saved best model with Val Loss: 2.7124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:21<00:00,  9.01it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Train Loss: 2.6994, Val Loss: 2.6543, Time: 7.56 min, LR: 0.000035\n",
      "Saved best model with Val Loss: 2.6543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:16<00:00,  9.11it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Train Loss: 2.6460, Val Loss: 2.6032, Time: 7.48 min, LR: 0.000030\n",
      "Saved best model with Val Loss: 2.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:14<00:00,  9.14it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 35.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Train Loss: 2.6005, Val Loss: 2.5533, Time: 7.48 min, LR: 0.000026\n",
      "Saved best model with Val Loss: 2.5533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:39<00:00,  8.65it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Train Loss: 2.5615, Val Loss: 2.5093, Time: 7.88 min, LR: 0.000021\n",
      "Saved best model with Val Loss: 2.5093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:37<00:00,  8.68it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Train Loss: 2.5210, Val Loss: 2.4739, Time: 7.85 min, LR: 0.000017\n",
      "Saved best model with Val Loss: 2.4739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:37<00:00,  8.69it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Train Loss: 2.4866, Val Loss: 2.4345, Time: 7.84 min, LR: 0.000014\n",
      "Saved best model with Val Loss: 2.4345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:40<00:00,  8.64it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Train Loss: 2.4597, Val Loss: 2.4080, Time: 7.89 min, LR: 0.000010\n",
      "Saved best model with Val Loss: 2.4080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:26<00:00,  8.90it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 38.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Train Loss: 2.4378, Val Loss: 2.3822, Time: 7.66 min, LR: 0.000008\n",
      "Saved best model with Val Loss: 2.3822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:23<00:00,  8.96it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 38.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Train Loss: 2.4188, Val Loss: 2.3630, Time: 7.60 min, LR: 0.000005\n",
      "Saved best model with Val Loss: 2.3630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:23<00:00,  8.96it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 38.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Train Loss: 2.4020, Val Loss: 2.3531, Time: 7.61 min, LR: 0.000003\n",
      "No improvement in Val Loss, count: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:26<00:00,  8.90it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Train Loss: 2.3913, Val Loss: 2.3435, Time: 7.65 min, LR: 0.000002\n",
      "Saved best model with Val Loss: 2.3435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:24<00:00,  8.94it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Train Loss: 2.3852, Val Loss: 2.3356, Time: 7.61 min, LR: 0.000001\n",
      "No improvement in Val Loss, count: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:19<00:00,  9.04it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Train Loss: 2.3810, Val Loss: 2.3318, Time: 7.53 min, LR: 0.000001\n",
      "Saved best model with Val Loss: 2.3318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:29<00:00,  8.83it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Train Loss: 2.3774, Val Loss: 2.3302, Time: 7.72 min, LR: 0.000001\n",
      "No improvement in Val Loss, count: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:34<00:00,  8.74it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:13<00:00, 37.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Train Loss: 2.3763, Val Loss: 2.3272, Time: 7.80 min, LR: 0.000002\n",
      "No improvement in Val Loss, count: 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3973/3973 [07:30<00:00,  8.83it/s]\n",
      "Validating: 100%|██████████| 497/497 [00:12<00:00, 39.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Train Loss: 2.3750, Val Loss: 2.3233, Time: 7.71 min, LR: 0.000003\n",
      "No improvement in Val Loss, count: 3/3\n",
      "Early stopping at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T08:15:06.229392Z",
     "start_time": "2025-07-08T08:11:16.838338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Vocabulary class (unchanged)\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption_list in captions.values() for caption in caption_list for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "# Dataset (unchanged)\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, augment=False):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.image_names = data['image_names']\n",
    "        self.captions_dict = pickle.load(open(caption_file, 'rb'))\n",
    "        self.augment = augment\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        captions = self.captions_dict.get(image_name, [[]])\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()] if self.augment and len(captions) > 1 else captions[0]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "# Custom collate function (unchanged)\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names\n",
    "\n",
    "# Transformer Model with Greedy Inference (to match best_transformer_model.pt)\n",
    "class ImageCaptionTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6, hidden_dim=2048, dropout=0.05, max_len=100):\n",
    "        super(ImageCaptionTransformer, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding(captions) * (self.embed_dim ** 0.5)\n",
    "        positions = torch.arange(0, captions.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embed = self.pos_encoder[:, :captions.size(1), :]\n",
    "        embed = embed + pos_embed\n",
    "        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
    "        output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def inference(self, features, max_len=20):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        outputs = []\n",
    "        input_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)\n",
    "        for _ in range(max_len):\n",
    "            embed = self.embedding(input_token) * (self.embed_dim ** 0.5)\n",
    "            positions = torch.arange(0, input_token.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "            pos_embed = self.pos_encoder[:, :input_token.size(1), :]\n",
    "            embed = embed + pos_embed\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_token.size(1)).to(device)\n",
    "            output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "            output = self.fc(output[:, -1, :])\n",
    "            _, next_token = output.max(1)\n",
    "            outputs.append(next_token)\n",
    "            input_token = torch.cat([input_token, next_token.unsqueeze(1)], dim=1)\n",
    "            if (next_token == 2).all():\n",
    "                break\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Load original captions\n",
    "def load_original_captions(caption_txt_file):\n",
    "    caption_dict = {}\n",
    "    try:\n",
    "        with open(caption_txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(caption_txt_file, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    return caption_dict\n",
    "\n",
    "# Evaluation function (updated for greedy decoding)\n",
    "def evaluate_model(model, loader, vocab, caption_dict, max_len=20, num_samples=5):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len)  # Greedy decoding\n",
    "            for i in range(captions.size(0)):\n",
    "                image_name = image_names[i]\n",
    "                ref = caption_dict.get(image_name, [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]])\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_name,\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    return bleu_score, sample_outputs\n",
    "\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "caption_txt_file = r'D:\\Projects\\AIniverse\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "# Load original captions\n",
    "caption_dict = load_original_captions(caption_txt_file)\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, augment=False)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, augment=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = pickle.load(open(os.path.join(feature_dir, 'vocab.pkl'), 'rb'))\n",
    "print(f\"Loaded vocab.pkl with {len(vocab.itos)} tokens\")\n",
    "\n",
    "# Initialize model\n",
    "model = ImageCaptionTransformer(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(vocab.itos),\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    hidden_dim=2048,\n",
    "    dropout=0.05,\n",
    "    max_len=100\n",
    ").cuda()\n",
    "\n",
    "# Load trained model weights\n",
    "model_path = os.path.join(feature_dir, 'best_transformer_model.pt')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_bleu, val_samples = evaluate_model(model, val_loader, vocab, caption_dict, max_len=20, num_samples=5)\n",
    "test_bleu, test_samples = evaluate_model(model, test_loader, vocab, caption_dict, max_len=20, num_samples=5)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nValidation BLEU-4 Score: {val_bleu:.4f}\")\n",
    "print(\"\\nValidation Sample Outputs:\")\n",
    "for sample in val_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")\n",
    "\n",
    "print(f\"\\nTest BLEU-4 Score: {test_bleu:.4f}\")\n",
    "print(\"\\nTest Sample Outputs:\")\n",
    "for sample in test_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")\n",
    "\n",
    "# Debug: Verify caption alignment\n",
    "print(\"\\nSample Caption.txt Verification:\")\n",
    "for i, image_name in enumerate(list(caption_dict.keys())[:5]):\n",
    "    print(f\"Image {i+1}: {image_name}, Caption: {' '.join(caption_dict[image_name])}\")\n",
    "\n",
    "# Debug: Verify vocabulary\n",
    "print(f\"\\nSample Vocab Tokens: {list(vocab.itos.items())[:10]}\")"
   ],
   "id": "765fd4f6d35c4e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab.pkl with 12097 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 497/497 [01:51<00:00,  4.44it/s]\n",
      "Evaluating: 100%|██████████| 497/497 [01:52<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation BLEU-4 Score: 0.0862\n",
      "\n",
      "Validation Sample Outputs:\n",
      "Image: 4420802292.jpg\n",
      "Ground Truth: A man concentrates to paint details .\n",
      "Generated: a man with a green cap and a white long sleeve shirt is painting a picture of a building with\n",
      "\n",
      "Image: 1579291454.jpg\n",
      "Ground Truth: Priests reading prayers from note cards .\n",
      "Generated: a man in a white robe is reading from a book to another man in a white robe\n",
      "\n",
      "Image: 3720366614.jpg\n",
      "Ground Truth: The dog is jumping up beside a red wall .\n",
      "Generated: a dog is jumping up to catch a toy while another dog watches\n",
      "\n",
      "Image: 2315113960.jpg\n",
      "Ground Truth: A woman at an exhibit faces away from a camera .\n",
      "Generated: a woman is looking at a photograph\n",
      "\n",
      "Image: 2785408815.jpg\n",
      "Ground Truth: a man smiling in a white coat .\n",
      "Generated: a man in a lab coat is smiling with a large smile on his face\n",
      "\n",
      "\n",
      "Test BLEU-4 Score: 0.0852\n",
      "\n",
      "Test Sample Outputs:\n",
      "Image: 3692746368.jpg\n",
      "Ground Truth: Two people are in a pond pulling a life raft .\n",
      "Generated: two people are in a muddy river fishing\n",
      "\n",
      "Image: 157139628.jpg\n",
      "Ground Truth: Man brushes image in the sand .\n",
      "Generated: a man is spraying a design into sand\n",
      "\n",
      "Image: 3205008852.jpg\n",
      "Ground Truth: A man and two horses .\n",
      "Generated: a man in a black hat loading up his gear on the mule\n",
      "\n",
      "Image: 2589156742.jpg\n",
      "Ground Truth: an african american man holds his soda .\n",
      "Generated: a black man wearing a hat and holding a cigarette\n",
      "\n",
      "Image: 2335129954.jpg\n",
      "Ground Truth: A little boy enjoying birds in the park .\n",
      "Generated: a little boy is playing in water with birds in the background\n",
      "\n",
      "\n",
      "Sample Caption.txt Verification:\n",
      "Image 1: 1000092795.jpg, Caption: Two friends enjoy time spent together .\n",
      "Image 2: 10002456.jpg, Caption: Three men on a large rig .\n",
      "Image 3: 1000268201.jpg, Caption: A girl going into a wooden building .\n",
      "Image 4: 1000344755.jpg, Caption: a man on a ladder cleans a window\n",
      "Image 5: 1000366164.jpg, Caption: Two men are cooking a meal .\n",
      "\n",
      "Sample Vocab Tokens: [(0, '<PAD>'), (1, '<SOS>'), (2, '<EOS>'), (3, '<UNK>'), (4, 'two'), (5, 'young'), (6, 'guys'), (7, 'with'), (8, 'shaggy'), (9, 'hair')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T08:59:45.503678Z",
     "start_time": "2025-07-08T08:59:43.851974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.load(os.path.join(feature_dir, 'train_vit_features.pt'), weights_only=False)\n",
    "features, image_names = data['features'], data['image_names']\n",
    "unique_indices = {name: i for i, name in enumerate(image_names)}.values()\n",
    "unique_features = features[list(unique_indices)]\n",
    "unique_image_names = [image_names[i] for i in unique_indices]\n",
    "torch.save({'features': unique_features, 'image_names': unique_image_names}, os.path.join(feature_dir, 'train_vit_features_unique.pt'))\n",
    "print(f\"Unique train features shape: {unique_features.shape}\")"
   ],
   "id": "78e98cd59e9e53cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train features shape: torch.Size([31774, 768])\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
