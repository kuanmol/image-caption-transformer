{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Image preprocessing pipeline\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    captions = [str(c).strip() for c in captions if not pd.isna(c)]\n",
    "    results = []\n",
    "    for i in range(0, len(captions), batch_size):\n",
    "        batch = captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([caption.lower() for caption in batch], disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "    return results\n",
    "\n",
    "# Dataset\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_transform=None):\n",
    "        self.df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "        self.df['caption'] = self.df['caption'].fillna('').astype(str)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = preprocess_image(img_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "        caption_tokens = preprocess_caption([caption])[0]\n",
    "        return image, caption_tokens, img_name\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None\n",
    "    images, captions, image_names = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, captions, image_names\n",
    "\n",
    "# Initialize dataset and splits\n",
    "image_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images'\n",
    "annotation_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "dataset = Flickr30kDataset(image_dir, annotation_file, image_transform)\n",
    "print(\"Dataset loaded, size:\", len(dataset))\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "print(\"Train subset size:\", len(train_subset))\n",
    "print(\"Val subset size:\", len(val_subset))\n",
    "print(\"Test subset size:\", len(test_subset))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Verify pipeline\n",
    "def show_sample(image, caption):\n",
    "    try:\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            raise ValueError(\"Image is not a torch.Tensor\")\n",
    "        if image.shape != (3, 224, 224):\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        image = np.clip(image, 0, 1)\n",
    "        if not isinstance(caption, list) or not all(isinstance(word, str) for word in caption):\n",
    "            raise ValueError(f\"Invalid caption format: {caption}\")\n",
    "        caption_text = ' '.join(caption).strip()\n",
    "        plt.imshow(image)\n",
    "        plt.title(caption_text)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in show_sample: {e}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, captions, _ = batch\n",
    "    if images is None or captions is None:\n",
    "        print(\"Skipping batch with None values\")\n",
    "        continue\n",
    "    print(f\"Image batch shape: {images.shape}\")\n",
    "    print(f\"Sample caption: {captions[0]}\")\n",
    "    show_sample(images[0], captions[0])\n",
    "    break\n",
    "\n",
    "# Load pre-trained ResNet-50\n",
    "resnet = models.resnet50(pretrained=True).cuda()\n",
    "resnet.eval()\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove final layer\n",
    "\n",
    "# Create directory for ResNet features\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Features'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "def extract_features(loader, split_name, subset_indices):\n",
    "    features = []\n",
    "    captions_list = []\n",
    "    image_names = []\n",
    "    for batch_idx, (images, captions, batch_image_names) in enumerate(loader):\n",
    "        if images is None:\n",
    "            continue\n",
    "        images = images.cuda()\n",
    "        with torch.no_grad():\n",
    "            batch_features = resnet(images).squeeze(-1).squeeze(-1)  # [batch_size, 2048]\n",
    "        features.append(batch_features.cpu())\n",
    "        captions_list.extend(captions)\n",
    "        image_names.extend(batch_image_names)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    torch.save({'features': features, 'captions': captions_list, 'image_names': image_names},\n",
    "               os.path.join(feature_dir, f'{split_name}_features.pt'))\n",
    "    print(f\"Saved {split_name} features: {features.shape}\")\n",
    "\n",
    "# Extract ResNet features\n",
    "extract_features(train_loader, 'train', train_indices)\n",
    "extract_features(val_loader, 'val', val_indices)\n",
    "extract_features(test_loader, 'test', test_indices)\n",
    "\n",
    "# Load pre-trained ViT\n",
    "vit = models.vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1).cuda()\n",
    "vit.heads = nn.Identity()  # Remove classification head\n",
    "vit.eval()\n",
    "\n",
    "# Create directory for ViT features\n",
    "vit_feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "os.makedirs(vit_feature_dir, exist_ok=True)\n",
    "\n",
    "def extract_vit_features(loader, split_name):\n",
    "    print(f\"Starting {split_name} feature extraction\")\n",
    "    features = []\n",
    "    captions_list = []\n",
    "    image_names = []\n",
    "    start_time = time.time()\n",
    "    with torch.amp.autocast('cuda'), torch.no_grad():\n",
    "        for batch_idx, (images, captions, batch_image_names) in enumerate(tqdm(loader, desc=f\"Processing {split_name}\")):\n",
    "            if images is None:\n",
    "                print(f\"Skipping batch {batch_idx + 1}: No images\")\n",
    "                continue\n",
    "            images = images.cuda()\n",
    "            batch_features = vit(images)  # [batch_size, 768]\n",
    "            features.append(batch_features.cpu())\n",
    "            captions_list.extend(captions)\n",
    "            image_names.extend(batch_image_names)\n",
    "        if features:\n",
    "            final_features = torch.cat(features, dim=0)\n",
    "        else:\n",
    "            final_features = torch.tensor([]).reshape(0, 768)\n",
    "        torchsave_path = os.path.join(vit_feature_dir, f'{split_name}_vit_features.pt')\n",
    "        torch.save({\n",
    "            'features': final_features,\n",
    "            'captions': captions_list,\n",
    "            'image_names': image_names\n",
    "        }, torchsave_path)\n",
    "        print(f\"Saved {split_name} ViT features: {final_features.shape}\")\n",
    "        print(f\"Time taken: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "# Extract ViT features\n",
    "extract_vit_features(train_loader, 'train')\n",
    "extract_vit_features(val_loader, 'val')\n",
    "extract_vit_features(test_loader, 'test')\n",
    "\n",
    "# Verify ViT features\n",
    "for split in ['train', 'val', 'test']:\n",
    "    file_path = os.path.join(vit_feature_dir, f'{split}_vit_features.pt')\n",
    "    if os.path.exists(file_path):\n",
    "        data = torch.load(file_path, weights_only=False)\n",
    "        print(f\"{split} features shape: {data['features'].shape}\")\n",
    "        print(f\"Sample caption: {data['captions'][0]}\")\n",
    "        print(f\"Sample image name: {data['image_names'][0]}\")\n",
    "    else:\n",
    "        print(f\"{split}_vit_features.pt not found\")\n",
    "\n",
    "# Preprocess and save captions\n",
    "df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "preprocessed_captions = preprocess_caption(df['caption'].tolist())\n",
    "pickle.dump(preprocessed_captions, open(r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl', 'wb'))\n",
    "\n",
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption_list in captions.values() for caption in caption_list for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "# Caption dataset\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, augment=False):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.image_names = data['image_names']\n",
    "        self.captions_dict = pickle.load(open(caption_file, 'rb'))\n",
    "        self.augment = augment\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        captions = self.captions_dict.get(image_name, [[]])\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()] if self.augment and len(captions) > 1 else captions[0]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "# Transformer model\n",
    "class ImageCaptionTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 hidden_dim=2048, dropout=0.05, max_len=100):\n",
    "        super(ImageCaptionTransformer, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding(captions) * (self.embed_dim ** 0.5)\n",
    "        positions = torch.arange(0, captions.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embed = self.pos_encoder[:, :captions.size(1), :]\n",
    "        embed = embed + pos_embed\n",
    "        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
    "        output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def inference(self, features, max_len=20, beam_size=3):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        sequences = [[torch.full((1, 1), 1, dtype=torch.long, device=device), 0.0]] * batch_size\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for i in range(batch_size):\n",
    "                candidates = []\n",
    "                for seq, score in sequences[i]:\n",
    "                    if seq[0, -1].item() == 2:\n",
    "                        candidates.append([seq, score])\n",
    "                        continue\n",
    "                    embed = self.embedding(seq) * (self.embed_dim ** 0.5)\n",
    "                    positions = torch.arange(0, seq.size(1), device=device).unsqueeze(0)\n",
    "                    pos_embed = self.pos_encoder[:, :seq.size(1), :]\n",
    "                    embed = embed + pos_embed\n",
    "                    tgt_mask = self.generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "                    output = self.transformer_decoder(embed, memory[i:i + 1], tgt_mask=tgt_mask)\n",
    "                    output = self.fc(output[:, -1, :])\n",
    "                    probs = torch.softmax(output, dim=-1)\n",
    "                    top_probs, top_idx = probs.topk(beam_size, dim=-1)\n",
    "                    for k in range(beam_size):\n",
    "                        next_token = top_idx[0, k].unsqueeze(0).unsqueeze(0)\n",
    "                        next_score = score - torch.log(top_probs[0, k]).item()\n",
    "                        new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                        candidates.append([new_seq, next_score])\n",
    "                candidates = sorted(candidates, key=lambda x: x[1])[:beam_size]\n",
    "                all_candidates.append(candidates)\n",
    "            sequences = all_candidates\n",
    "        outputs = [sequences[i][0][0] for i in range(batch_size)]\n",
    "        return torch.cat(outputs, dim=0)\n",
    "\n",
    "# Load original captions\n",
    "def load_original_captions(caption_txt_file):\n",
    "    caption_dict = {}\n",
    "    try:\n",
    "        with open(caption_txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(caption_txt_file, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    return caption_dict\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, vocab, caption_dict, max_len=20, num_samples=5, beam_size=3):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len, beam_size=beam_size)\n",
    "            for i in range(captions.size(0)):\n",
    "                image_name = image_names[i]\n",
    "                ref = caption_dict.get(image_name,\n",
    "                                       [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]])\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_name,\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    return bleu_score, sample_outputs\n",
    "\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "caption_txt_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'train_vit_features.pt'), caption_file, augment=True)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, augment=False)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Save vocab\n",
    "with open(os.path.join(feature_dir, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_dataset.vocab, f)\n",
    "print(f\"Saved vocab.pkl with {len(train_dataset.vocab.itos)} tokens\")\n",
    "\n",
    "# Model\n",
    "model = ImageCaptionTransformer(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(train_dataset.vocab.itos),\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    hidden_dim=2048,\n",
    "    dropout=0.05,\n",
    "    max_len=100\n",
    ").cuda()\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, captions, _ in tqdm(loader, desc=\"Training\"):\n",
    "        if features is None:\n",
    "            continue\n",
    "        features, captions = features.cuda(), captions.cuda()\n",
    "        outputs = model(features, captions, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Validation loop\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, captions, _ in tqdm(loader, desc=\"Validating\"):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model(features, captions, teacher_forcing_ratio=1.0)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_count = 0\n",
    "min_delta = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {(time.time() - start_time) / 60:.2f} min, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), os.path.join(feature_dir, 'best_transformer_model.pt'))\n",
    "        print(f\"Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f\"No improvement in Val Loss, count: {no_improve_count}/{patience}\")\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
