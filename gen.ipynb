{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ViT_B_16_Weights\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import spacy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Define image preprocessing pipeline\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ],
   "id": "8277fac6b40334d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "image_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images'\n",
    "annotation_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "# Check images\n",
    "print(f\"Images found: {len(os.listdir(image_dir))}\")  # Expected: ~31,783\n",
    "# Check captions\n",
    "df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "print(f\"Captions found: {len(df)}\")  # Expected: ~158,915\n",
    "print(df.head())  # Should show image-caption pairs like your example"
   ],
   "id": "3dc6672ec827a842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "tensor = preprocess_image(r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images\\36979.jpg')\n",
    "if tensor is not None:\n",
    "    print(tensor.shape)  # Expected: torch.Size([3, 224, 224])\n",
    "\n",
    "tensor = preprocess_image(r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images\\36979.jpg')\n",
    "if tensor is not None:\n",
    "    torchvision.utils.save_image(tensor, 'preprocessed_image.jpg')"
   ],
   "id": "1ab05c7c3ced0123",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load spaCy model once globally\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "# Use this updated and safe function\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    # Normalize to list\n",
    "    if captions is None or isinstance(captions, float):\n",
    "        captions = ['']\n",
    "    elif isinstance(captions, str):\n",
    "        captions = [captions]\n",
    "    elif not isinstance(captions, list):\n",
    "        raise TypeError(f\"Expected string or list of strings, got {type(captions)}\")\n",
    "\n",
    "    # Strip and lowercase all valid entries\n",
    "    clean_captions = [str(c).strip() for c in captions if isinstance(c, (str, float, int))]\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(clean_captions), batch_size):\n",
    "        batch = clean_captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([text.lower() for text in batch], disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "\n",
    "    return results if len(results) > 1 else results[0]\n",
    "\n",
    "\n",
    "# Test\n",
    "caption = \"A dog is running in the park.\"\n",
    "print(preprocess_caption(caption))  # Expected: ['a', 'dog', 'is', 'running', 'in', 'the', 'park']\n",
    "captions = [\"A dog is running in the park.\", \"A cat sleeps on a mat.\"]\n",
    "print(preprocess_caption(\n",
    "    captions))  # Expected: [['a', 'dog', 'is', 'running', 'in', 'the', 'park'], ['a', 'cat', 'sleeps', 'on', 'a', 'mat']]"
   ],
   "id": "1244244df9cb8aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_transform=None):\n",
    "        self.df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        caption = row['caption']\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = preprocess_image(img_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "\n",
    "        caption_tokens = preprocess_caption(caption)\n",
    "        return image, caption_tokens\n",
    "\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None\n",
    "    images, captions = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, captions\n",
    "\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = Flickr30kDataset(\n",
    "    image_dir=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images',\n",
    "    annotation_file=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt',\n",
    "    image_transform=image_transform\n",
    ")"
   ],
   "id": "dcac7d6f29222f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Create splits\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ],
   "id": "2fd30f4074da10f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Verify pipeline\n",
    "def show_sample(image, caption):\n",
    "    try:\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            raise ValueError(\"Image is not a torch.Tensor\")\n",
    "        if image.shape != (3, 224, 224):\n",
    "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        image = image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "        image = np.clip(image, 0, 1)\n",
    "        if not isinstance(caption, list) or not all(isinstance(word, str) for word in caption):\n",
    "            raise ValueError(f\"Invalid caption format: {caption}\")\n",
    "        caption_text = ' '.join(caption).strip()\n",
    "        plt.imshow(image)\n",
    "        plt.title(caption_text)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in show_sample: {e}\")\n",
    "\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, captions = batch\n",
    "    if images is None or captions is None:\n",
    "        print(\"Skipping batch with None values\")\n",
    "        continue\n",
    "    print(f\"Image batch shape: {images.shape}\")  # Expected: [32, 3, 224, 224]\n",
    "    print(f\"Sample caption: {captions[0]}\")  # Expected: ['a', 'dog', 'has', ...]\n",
    "    show_sample(images[0], captions[0])\n",
    "    break"
   ],
   "id": "609f0a88eadf2446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    # Normalize to list\n",
    "    if captions is None or isinstance(captions, float):\n",
    "        captions = ['']\n",
    "    elif isinstance(captions, str):\n",
    "        captions = [captions]\n",
    "    elif not isinstance(captions, list):\n",
    "        raise TypeError(f\"Expected string or list, got {type(captions)}\")\n",
    "\n",
    "    # Strip and filter\n",
    "    clean_captions = [str(c).strip() for c in captions\n",
    "                      if isinstance(c, (str, float, int))]\n",
    "\n",
    "    results = []\n",
    "    for i in range(0, len(clean_captions), batch_size):\n",
    "        batch = clean_captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([text.lower() for text in batch],\n",
    "                        disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text\n",
    "                      for token in doc\n",
    "                      if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "\n",
    "    return results if len(results) > 1 else results[0]\n",
    "\n",
    "\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_transform=None):\n",
    "        self.df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        caption = row['caption']\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = preprocess_image(img_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "\n",
    "        caption_tokens = preprocess_caption(caption)\n",
    "        return image, caption_tokens, img_name  # Return image name\n",
    "\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None\n",
    "    images, captions, image_names = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, captions, image_names\n",
    "\n",
    "\n",
    "# Initialize dataset and splits\n",
    "dataset = Flickr30kDataset(\n",
    "    image_dir=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images',\n",
    "    annotation_file=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt',\n",
    "    image_transform=image_transform\n",
    ")\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)"
   ],
   "id": "29ffd61627cb789",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Load pre-trained ResNet-50\n",
    "resnet = models.resnet50(pretrained=True).cuda()\n",
    "resnet.eval()\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Remove final layer\n",
    "\n",
    "# Create directory for features\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Features'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Fixed extract_features\n",
    "def extract_features(loader, split_name, subset_indices):\n",
    "    features = []\n",
    "    captions_list = []\n",
    "    image_names = []\n",
    "    for batch_idx, (images, captions, batch_image_names) in enumerate(loader):\n",
    "        if images is None:\n",
    "            continue\n",
    "        images = images.cuda()\n",
    "        with torch.no_grad():\n",
    "            batch_features = resnet(images).squeeze(-1).squeeze(-1)  # [batch_size, 2048]\n",
    "        features.append(batch_features.cpu())\n",
    "        captions_list.extend(captions)\n",
    "        image_names.extend(batch_image_names)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    torch.save({'features': features, 'captions': captions_list, 'image_names': image_names},\n",
    "               os.path.join(feature_dir, f'{split_name}_features.pt'))\n",
    "    print(f\"Saved {split_name} features: {features.shape}\")\n",
    "\n",
    "\n",
    "# Extract features\n",
    "extract_features(train_loader, 'train', train_indices)\n",
    "extract_features(val_loader, 'val', val_indices)\n",
    "extract_features(test_loader, 'test', test_indices)\n",
    "\n",
    "# Verify\n",
    "train_features = torch.load(os.path.join(feature_dir, 'train_features.pt'))\n",
    "print(f\"Train feature shape: {train_features['features'].shape}\")  # Expected: [127132, 2048]\n",
    "print(f\"Sample caption: {train_features['captions'][0]}\")  # Expected: ['a', 'dog', 'has', ...]\n",
    "print(f\"Sample image name: {train_features['image_names'][0]}\")  # Expected: e.g., 1000092795.jpg"
   ],
   "id": "362c7d9efcb5e048",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Features'\n",
    "\n",
    "# Check if files exist\n",
    "for split in ['train', 'val', 'test']:\n",
    "    file_path = os.path.join(feature_dir, f'{split}_features.pt')\n",
    "    if os.path.exists(file_path):\n",
    "        data = torch.load(file_path)\n",
    "        print(f\"{split} features shape: {data['features'].shape}\")\n",
    "        print(f\"Sample caption: {data['captions'][0]}\")\n",
    "        print(f\"Sample image name: {data['image_names'][0]}\")\n",
    "    else:\n",
    "        print(f\"{split}_features.pt not found\")"
   ],
   "id": "4b862b0e2ad00bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Image preprocessing\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return image_transform(image)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Caption preprocessing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    if isinstance(captions, (str, float)):\n",
    "        captions = [str(captions)]\n",
    "    captions = [str(c).strip() for c in captions]\n",
    "    results = []\n",
    "    for i in range(0, len(captions), batch_size):\n",
    "        batch = captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([caption.lower() for caption in batch], disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "    return results if len(results) > 1 else results[0]\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class Flickr30kDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, image_transform=None):\n",
    "        self.df = pd.read_csv(annotation_file, sep=',', names=['image', 'caption'], skiprows=1)\n",
    "        self.df['caption'] = self.df['caption'].fillna('').astype(str)\n",
    "        self.image_dir = image_dir\n",
    "        self.image_transform = image_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row['image']\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = preprocess_image(img_path)\n",
    "        if image is None:\n",
    "            return None\n",
    "        caption_tokens = preprocess_caption(caption)\n",
    "        return image, caption_tokens, img_name\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if len(batch) == 0:\n",
    "        return None, None, None\n",
    "    images, captions, image_names = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, captions, image_names\n",
    "\n",
    "\n",
    "# Initialize dataset and splits\n",
    "dataset = Flickr30kDataset(\n",
    "    image_dir=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\Images',\n",
    "    annotation_file=r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt',\n",
    "    image_transform=image_transform\n",
    ")\n",
    "print(\"Dataset loaded, size:\", len(dataset))\n",
    "indices = np.arange(len(dataset))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(indices)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "val_subset = Subset(dataset, val_indices)\n",
    "test_subset = Subset(dataset, test_indices)\n",
    "print(\"Train subset size:\", len(train_subset))\n",
    "print(\"Val subset size:\", len(val_subset))\n",
    "print(\"Test subset size:\", len(test_subset))\n",
    "\n",
    "# Load pre-trained ViT and modify to get feature embeddings\n",
    "vit = models.vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1).cuda()\n",
    "vit.heads = nn.Identity()  # Remove classification head to get [batch_size, 768]\n",
    "vit.eval()\n",
    "\n",
    "# Create directory for ViT features\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Extract ViT features\n",
    "def extract_vit_features(loader, split_name):\n",
    "    print(f\"Starting {split_name} feature extraction\")\n",
    "    features = []\n",
    "    captions_list = []\n",
    "    image_names = []\n",
    "    start_time = time.time()\n",
    "    with torch.amp.autocast('cuda'), torch.no_grad():\n",
    "        for batch_idx, (images, captions, batch_image_names) in enumerate(\n",
    "                tqdm(loader, desc=f\"Processing {split_name}\")):\n",
    "            if images is None:\n",
    "                print(f\"Skipping batch {batch_idx + 1}: No images\")\n",
    "                continue\n",
    "            images = images.cuda()\n",
    "            batch_features = vit(images)  # [batch_size, 768]\n",
    "            features.append(batch_features.cpu())\n",
    "            captions_list.extend(captions)\n",
    "            image_names.extend(batch_image_names)\n",
    "        if features:\n",
    "            final_features = torch.cat(features, dim=0)\n",
    "        else:\n",
    "            final_features = torch.tensor([]).reshape(0, 768)\n",
    "        torchsave_path = os.path.join(feature_dir, f'{split_name}_vit_features.pt')\n",
    "        torch.save({\n",
    "            'features': final_features,\n",
    "            'captions': captions_list,\n",
    "            'image_names': image_names\n",
    "        }, torchsave_path)\n",
    "        print(f\"Saved {split_name} ViT features: {final_features.shape}\")\n",
    "        print(f\"Time taken: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "\n",
    "# DataLoader with batch_size=32\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_subset, batch_size=32, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Extract features\n",
    "extract_vit_features(train_loader, 'train')\n",
    "extract_vit_features(val_loader, 'val')\n",
    "extract_vit_features(test_loader, 'test')\n",
    "\n",
    "# Verify\n",
    "for split in ['train', 'val', 'test']:\n",
    "    file_path = os.path.join(feature_dir, f'{split}_vit_features.pt')\n",
    "    if os.path.exists(file_path):\n",
    "        data = torch.load(file_path, weights_only=False)\n",
    "        print(f\"{split} features shape: {data['features'].shape}\")\n",
    "        print(f\"Sample caption: {data['captions'][0]}\")\n",
    "        print(f\"Sample image name: {data['image_names'][0]}\")\n",
    "    else:\n",
    "        print(f\"{split}_vit_features.pt not found\")"
   ],
   "id": "71907d38d64d95b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "\n",
    "# Check if files exist\n",
    "for split in ['train', 'val', 'test']:\n",
    "    file_path = os.path.join(feature_dir, f'{split}_vit_features.pt')\n",
    "    if os.path.exists(file_path):\n",
    "        data = torch.load(file_path)\n",
    "        print(f\"{split} features shape: {data['features'].shape}\")\n",
    "        print(f\"Sample caption: {data['captions'][0]}\")\n",
    "        print(f\"Sample image name: {data['image_names'][0]}\")\n",
    "    else:\n",
    "        print(f\"{split}_features.pt not found\")"
   ],
   "id": "4250e8c82c2a28ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def preprocess_caption(captions, batch_size=1000):\n",
    "    captions = [str(c).strip() for c in captions if not pd.isna(c)]\n",
    "    results = []\n",
    "    for i in range(0, len(captions), batch_size):\n",
    "        batch = captions[i:i + batch_size]\n",
    "        docs = nlp.pipe([caption.lower() for caption in batch], disable=['parser', 'ner'])\n",
    "        for doc in docs:\n",
    "            tokens = [token.text for token in doc if not token.is_punct and token.text.strip()]\n",
    "            results.append(tokens)\n",
    "    return results\n",
    "\n",
    "\n",
    "df = pd.read_csv(r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt', sep=',', names=['image', 'caption'], skiprows=1)\n",
    "preprocessed_captions = preprocess_caption(df['caption'].tolist())\n",
    "pickle.dump(preprocessed_captions,open(r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl','wb'))\n",
    "\n"
   ],
   "id": "cea9dda3c9668028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRANSFORMER",
   "id": "f73eecd84c84a782"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Vocabulary class\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption_list in captions.values() for caption in caption_list for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]"
   ],
   "id": "7fe592f6db1a1100",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Dataset\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, augment=False):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.image_names = data['image_names']\n",
    "        self.captions_dict = pickle.load(open(caption_file, 'rb'))\n",
    "        self.augment = augment\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        captions = self.captions_dict.get(image_name, [[]])\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()] if self.augment and len(captions) > 1 else \\\n",
    "            captions[0]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names"
   ],
   "id": "ccdb97ecdc78620f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Transformer Model with Beam Search\n",
    "class ImageCaptionTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 hidden_dim=2048, dropout=0.05, max_len=100):\n",
    "        super(ImageCaptionTransformer, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, embed_dim)  #Projects the image feature vector into Transformer embedding space.\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) #Converts word indices into dense vectors.\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_len, embed_dim)) #Learnable positional encoding\n",
    "\n",
    "        #A stack of num_encoder_layers self-attention + feed-forward blocks, refine and contextualize the image representation using multi-head attention.\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size) #Final output projection to vocabulary logits.\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        #The encoder transforms the raw image vector into a context-aware vector (memory) that the decoder can attend to.\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding(captions) * (self.embed_dim ** 0.5)\n",
    "        positions = torch.arange(0, captions.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embed = self.pos_encoder[:, :captions.size(1), :]\n",
    "        embed = embed + pos_embed\n",
    "        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
    "        output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def inference(self, features, max_len=20, beam_size=3):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        sequences = [[torch.full((1, 1), 1, dtype=torch.long, device=device), 0.0]] * batch_size\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            for i in range(batch_size):\n",
    "                candidates = []\n",
    "                for seq, score in sequences[i]:\n",
    "                    if seq[0, -1].item() == 2:\n",
    "                        candidates.append([seq, score])\n",
    "                        continue\n",
    "                    embed = self.embedding(seq) * (self.embed_dim ** 0.5)\n",
    "                    positions = torch.arange(0, seq.size(1), device=device).unsqueeze(0)\n",
    "                    pos_embed = self.pos_encoder[:, :seq.size(1), :]\n",
    "                    embed = embed + pos_embed\n",
    "                    tgt_mask = self.generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "                    output = self.transformer_decoder(embed, memory[i:i + 1], tgt_mask=tgt_mask)\n",
    "                    output = self.fc(output[:, -1, :])\n",
    "                    probs = torch.softmax(output, dim=-1)\n",
    "                    top_probs, top_idx = probs.topk(beam_size, dim=-1)\n",
    "                    for k in range(beam_size):\n",
    "                        next_token = top_idx[0, k].unsqueeze(0).unsqueeze(0)\n",
    "                        next_score = score - torch.log(top_probs[0, k]).item()\n",
    "                        new_seq = torch.cat([seq, next_token], dim=1)\n",
    "                        candidates.append([new_seq, next_score])\n",
    "                candidates = sorted(candidates, key=lambda x: x[1])[:beam_size]\n",
    "                all_candidates.append(candidates)\n",
    "            sequences = all_candidates\n",
    "        outputs = [sequences[i][0][0] for i in range(batch_size)]\n",
    "        return torch.cat(outputs, dim=0)"
   ],
   "id": "47f618fb1890da6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Load original captions\n",
    "def load_original_captions(caption_txt_file):\n",
    "    caption_dict = {}\n",
    "    try:\n",
    "        with open(caption_txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(caption_txt_file, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    return caption_dict\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, vocab, caption_dict, max_len=20, num_samples=5, beam_size=3):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len, beam_size=beam_size)\n",
    "            for i in range(captions.size(0)):\n",
    "                image_name = image_names[i]\n",
    "                ref = caption_dict.get(image_name,\n",
    "                                       [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]])\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_name,\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    return bleu_score, sample_outputs"
   ],
   "id": "109ebd8d7a8d59ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "caption_txt_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'train_vit_features.pt'), caption_file, augment=True)\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, augment=False)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Save vocab\n",
    "with open(os.path.join(feature_dir, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(train_dataset.vocab, f)\n",
    "print(f\"Saved vocab.pkl with {len(train_dataset.vocab.itos)} tokens\")\n",
    "\n",
    "# Model\n",
    "model = ImageCaptionTransformer(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(train_dataset.vocab.itos),\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    hidden_dim=2048,\n",
    "    dropout=0.05,\n",
    "    max_len=100\n",
    ").cuda()"
   ],
   "id": "b1cacbd15540d352",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_epoch(model, loader, criterion, optimizer, teacher_forcing_ratio=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for features, captions, _ in tqdm(loader, desc=\"Training\"):\n",
    "        if features is None:\n",
    "            continue\n",
    "        features, captions = features.cuda(), captions.cuda()\n",
    "        outputs = model(features, captions, teacher_forcing_ratio)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# Validation loop\n",
    "def validate_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, captions, _ in tqdm(loader, desc=\"Validating\"):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model(features, captions, teacher_forcing_ratio=1.0)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "# Training with early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_count = 0\n",
    "min_delta = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, teacher_forcing_ratio=0.5)\n",
    "    val_loss = validate_epoch(model, val_loader, criterion)\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Time: {(time.time() - start_time) / 60:.2f} min, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    if val_loss < best_val_loss - min_delta:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_count = 0\n",
    "        torch.save(model.state_dict(), os.path.join(feature_dir, 'best_transformer_model.pt'))\n",
    "        print(f\"Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        no_improve_count += 1\n",
    "        print(f\"No improvement in Val Loss, count: {no_improve_count}/{patience}\")\n",
    "    if no_improve_count >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "# Epoch 33/100, Train Loss: 2.3750, Val Loss: 2.3233, Time: 7.71 min, LR: 0.000003\n",
    "# No improvement in Val Loss, count: 3/3\n",
    "# Early stopping at epoch 33"
   ],
   "id": "1a8e8083494d0e6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing evaluate",
   "id": "1a0ca8fab283d10d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# Vocabulary class (unchanged)\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def build_vocabulary(self, captions):\n",
    "        words = [word for caption_list in captions.values() for caption in caption_list for word in caption]\n",
    "        word_counts = Counter(words)\n",
    "        idx = len(self.itos)\n",
    "        for word, count in word_counts.items():\n",
    "            if count >= self.min_freq and word not in self.stoi:\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, caption):\n",
    "        return [self.stoi.get(word, self.stoi['<UNK>']) for word in caption]\n",
    "\n",
    "\n",
    "# Dataset (unchanged)\n",
    "class Flickr30kCaptionDataset(Dataset):\n",
    "    def __init__(self, feature_file, caption_file, augment=False):\n",
    "        data = torch.load(feature_file, weights_only=False)\n",
    "        self.features = data['features']\n",
    "        self.image_names = data['image_names']\n",
    "        self.captions_dict = pickle.load(open(caption_file, 'rb'))\n",
    "        self.augment = augment\n",
    "        self.vocab = Vocabulary(min_freq=2)\n",
    "        self.vocab.build_vocabulary(self.captions_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        image_name = self.image_names[idx]\n",
    "        captions = self.captions_dict.get(image_name, [[]])\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()] if self.augment and len(captions) > 1 else \\\n",
    "            captions[0]\n",
    "        numerical_caption = [self.vocab.stoi['<SOS>']] + self.vocab.numericalize(caption) + [self.vocab.stoi['<EOS>']]\n",
    "        return feature, torch.tensor(numerical_caption, dtype=torch.long), image_name\n",
    "\n",
    "\n",
    "# Custom collate function (unchanged)\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None, None, None\n",
    "    features, captions, image_names = zip(*batch)\n",
    "    features = torch.stack(features)\n",
    "    max_len = max(len(c) for c in captions)\n",
    "    padded_captions = torch.zeros(len(captions), max_len, dtype=torch.long)\n",
    "    for i, cap in enumerate(captions):\n",
    "        padded_captions[i, :len(cap)] = cap\n",
    "    return features, padded_captions, image_names\n",
    "\n",
    "\n",
    "# Transformer Model with Greedy Inference (to match best_transformer_model.pt)\n",
    "class ImageCaptionTransformer(nn.Module):\n",
    "    def __init__(self, feature_dim, vocab_size, embed_dim=512, num_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n",
    "                 hidden_dim=2048, dropout=0.05, max_len=100):\n",
    "        super(ImageCaptionTransformer, self).__init__()\n",
    "        self.feature_fc = nn.Linear(feature_dim, embed_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,\n",
    "                                                   dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_len = max_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.feature_fc.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.feature_fc.bias)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask\n",
    "\n",
    "    def forward(self, features, captions=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        if captions is None:\n",
    "            return self.inference(features)\n",
    "        captions = captions[:, :-1]\n",
    "        embed = self.embedding(captions) * (self.embed_dim ** 0.5)\n",
    "        positions = torch.arange(0, captions.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "        pos_embed = self.pos_encoder[:, :captions.size(1), :]\n",
    "        embed = embed + pos_embed\n",
    "        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(device)\n",
    "        output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def inference(self, features, max_len=20):\n",
    "        batch_size = features.size(0)\n",
    "        device = features.device\n",
    "        features = self.dropout(torch.relu(self.feature_fc(features)))\n",
    "        features = features.unsqueeze(1)\n",
    "        memory = self.transformer_encoder(features)\n",
    "        outputs = []\n",
    "        input_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)\n",
    "        for _ in range(max_len):\n",
    "            embed = self.embedding(input_token) * (self.embed_dim ** 0.5)\n",
    "            positions = torch.arange(0, input_token.size(1), device=device).unsqueeze(0).repeat(batch_size, 1)\n",
    "            pos_embed = self.pos_encoder[:, :input_token.size(1), :]\n",
    "            embed = embed + pos_embed\n",
    "            tgt_mask = self.generate_square_subsequent_mask(input_token.size(1)).to(device)\n",
    "            output = self.transformer_decoder(embed, memory, tgt_mask=tgt_mask)\n",
    "            output = self.fc(output[:, -1, :])\n",
    "            _, next_token = output.max(1)\n",
    "            outputs.append(next_token)\n",
    "            input_token = torch.cat([input_token, next_token.unsqueeze(1)], dim=1)\n",
    "            if (next_token == 2).all():\n",
    "                break\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "# Load original captions\n",
    "def load_original_captions(caption_txt_file):\n",
    "    caption_dict = {}\n",
    "    try:\n",
    "        with open(caption_txt_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(caption_txt_file, 'r', encoding='latin1') as f:\n",
    "            lines = f.readlines()[1:]  # Skip header\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_name, caption = parts\n",
    "                    caption_dict[image_name] = caption.split()\n",
    "    return caption_dict\n",
    "\n",
    "\n",
    "# Evaluation function (updated for greedy decoding)\n",
    "def evaluate_model(model, loader, vocab, caption_dict, max_len=20, num_samples=5):\n",
    "    model.eval()\n",
    "    references, hypotheses = [], []\n",
    "    sample_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, captions, image_names) in enumerate(tqdm(loader, desc=\"Evaluating\")):\n",
    "            if features is None:\n",
    "                continue\n",
    "            features, captions = features.cuda(), captions.cuda()\n",
    "            outputs = model.inference(features, max_len=max_len)  # Greedy decoding\n",
    "            for i in range(captions.size(0)):\n",
    "                image_name = image_names[i]\n",
    "                ref = caption_dict.get(image_name,\n",
    "                                       [vocab.itos[idx.item()] for idx in captions[i] if idx.item() not in [0, 1, 2]])\n",
    "                hyp = [vocab.itos[idx.item()] for idx in outputs[i] if idx.item() not in [0, 1, 2]]\n",
    "                references.append([ref])\n",
    "                hypotheses.append(hyp)\n",
    "                if len(sample_outputs) < num_samples and batch_idx * captions.size(0) + i < num_samples:\n",
    "                    sample_outputs.append({\n",
    "                        'image_name': image_name,\n",
    "                        'ground_truth': ' '.join(ref),\n",
    "                        'generated': ' '.join(hyp)\n",
    "                    })\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    return bleu_score, sample_outputs\n",
    "\n",
    "\n",
    "# Paths\n",
    "feature_dir = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\ViT_Features'\n",
    "caption_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\preprocessed_captions1.pkl'\n",
    "caption_txt_file = r'D:\\Projects\\AIniverse\\PROJECTS\\mainstream\\Scene_Description_Generator\\flickr30k\\versions\\1\\captions.txt'\n",
    "\n",
    "\n",
    "# Load original captions\n",
    "caption_dict = load_original_captions(caption_txt_file)\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "val_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'val_vit_features.pt'), caption_file, augment=False)\n",
    "test_dataset = Flickr30kCaptionDataset(os.path.join(feature_dir, 'test_vit_features.pt'), caption_file, augment=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = pickle.load(open(os.path.join(feature_dir, 'vocab.pkl'), 'rb'))\n",
    "print(f\"Loaded vocab.pkl with {len(vocab.itos)} tokens\")\n",
    "\n",
    "# Initialize model\n",
    "model = ImageCaptionTransformer(\n",
    "    feature_dim=768,\n",
    "    vocab_size=len(vocab.itos),\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    hidden_dim=2048,\n",
    "    dropout=0.05,\n",
    "    max_len=100\n",
    ").cuda()\n",
    "\n",
    "# Load trained model weights\n",
    "model_path = os.path.join(feature_dir, 'best_transformer_model.pt')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_bleu, val_samples = evaluate_model(model, val_loader, vocab, caption_dict, max_len=20, num_samples=10)\n",
    "test_bleu, test_samples = evaluate_model(model, test_loader, vocab, caption_dict, max_len=20, num_samples=10)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nValidation BLEU-4 Score: {val_bleu:.4f}\")\n",
    "print(\"\\nValidation Sample Outputs:\")\n",
    "for sample in val_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")\n",
    "\n",
    "for sample in test_samples:\n",
    "    print(f\"Image: {sample['image_name']}\")\n",
    "    print(f\"Ground Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Generated: {sample['generated']}\\n\")\n",
    "\n",
    "# Debug: Verify caption alignment\n",
    "print(\"\\nSample Caption.txt Verification:\")\n",
    "for i, image_name in enumerate(list(caption_dict.keys())[:5]):\n",
    "    print(f\"Image {i + 1}: {image_name}, Caption: {' '.join(caption_dict[image_name])}\")\n",
    "\n",
    "# Debug: Verify vocabulary\n",
    "print(f\"\\nSample Vocab Tokens: {list(vocab.itos.items())[:10]}\")"
   ],
   "id": "b75148b368c1da0c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
